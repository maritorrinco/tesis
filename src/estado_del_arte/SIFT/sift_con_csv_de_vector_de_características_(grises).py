# -*- coding: utf-8 -*-
"""SIFT con CSV de Vector de Características (Grises).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RXygLJKvKZdY0PtF_mSWNjujzkBh3kKa
"""

from google.colab import drive
drive.mount('/content/drive')

#Librerías
!pip3 install opencv-python==3.4.2.16
!pip3 install opencv-contrib-python==3.4.2.16
import cv2
from google.colab.patches import cv2_imshow
import glob
import numpy as np
from sklearn.svm import SVC
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
import csv

# Constantes
PATH_BASE = 'drive/My Drive/Colab Notebooks/Outex_TC_00013/'
noClusters = 50

# Funciones
def read_images(txt): # txt: archivo.txt
  labels = np.array([])

  # Lectura de nombres de archivos de entrenamiento
  path_base = PATH_BASE
  f = open(path_base + "000/" + txt,"r")
  lineas = f.readlines()

  imagenes = []
  for i in range(1, len(lineas)):
    nombreArchivo = lineas[i].split()[0]
    labels = np.append(labels, lineas[i].split()[1])
    path = path_base + "images/" + nombreArchivo
    img = cv2.imread(path)
    imagenes.append(img)
  return imagenes, labels # retorna un array de imágenes y un array de nombre de clases

# Lectura de imágenes de entrenamiento del archivo train.txt
imagenes_entrenamiento, labels_entrenamiento = read_images('train.txt')

listaDescriptores = []
for i in range(len(imagenes_entrenamiento)):
  imagen = imagenes_entrenamiento[i]

  # Extracción de keypoints con SIFT de las imágenes de entrenamiento
  escalaGrises = cv2.cvtColor(imagen, cv2.COLOR_BGR2GRAY)
  sift = cv2.xfeatures2d_SIFT.create()
  keypoints, des = sift.detectAndCompute(escalaGrises, None)

  listaDescriptores.append(des)
    
print(len(listaDescriptores))

# Inicilizar "descriptores"
descriptores = np.array(listaDescriptores[0])
print(type(descriptores))
print(descriptores)

# Cargar "descriptores"
i = 1
for descriptor in listaDescriptores[1:]:
  i = i + 1
  if (descriptor is None):
    descriptor = np.array([], dtype=np.int64).reshape(0,128)
  descriptores = np.vstack((descriptores, descriptor)) 

print("Imágenes procesadas: ", i)
print("Dimensiones de 'descriptores':", descriptores.shape)
print(descriptores)

kmeans = KMeans(init="random", n_clusters = noClusters).fit(descriptores)

cantImagenes =  len(listaDescriptores)
# Extraer keypoints por cada imagen
im_features = np.array([np.zeros(noClusters) for i in range(cantImagenes)])
print("Tamaño de im_features (imágenes con decriptores x número de Clústers):", im_features.shape)
for i in range(cantImagenes):
    cantDescriptoresImagen = 0 if listaDescriptores[i] is None else len(listaDescriptores[i])
    for j in range(cantDescriptoresImagen):
        feature = listaDescriptores[i][j]
        feature = feature.reshape(1, 128) # Predecir el clúster más cercano al que pertenece cada muestra de X.
        # A qué cluster pertenece
        idx = kmeans.predict(feature)
        im_features[i][idx] += 1

# Normalizar características
scale = StandardScaler().fit(im_features)        
im_features = scale.transform(im_features)

# Entrenar clasificador
svm = SVC(kernel = 'linear')
svm.fit(im_features, labels_entrenamiento)

CSV_ENTRENAMIENTO = "/content/drive/MyDrive/Colab Notebooks/resultados/Outex13_grises_50_entrenamiento.csv"

with open(CSV_ENTRENAMIENTO,'w+') as f:
    w = csv.writer(f)
    titulos = ["CLASE"] + list(range(noClusters))
    w.writerow(titulos)
    for i in range(len(im_features)):
      fila = []
      fila.append(labels_entrenamiento[i])
      fila = fila + list(im_features[0])
      w.writerow(fila)

# Lectura de imágenes de prueba del archivo test.txt
imagenes_prueba, labels_prueba = read_images('test.txt')

listaDescriptoresPrueba = []
for i in range(len(imagenes_prueba)):
  imagen = imagenes_prueba[i]

  # Extracción de keypoints con SIFT de las imágenes de prueba
  escalaGrises = cv2.cvtColor(imagen, cv2.COLOR_BGR2GRAY)
  sift = cv2.xfeatures2d_SIFT.create()
  keypoints, des = sift.detectAndCompute(escalaGrises, None)

  listaDescriptoresPrueba.append(des)
    
print(len(listaDescriptoresPrueba))

# Asingnar cada keypoint a un clúster
cantImagenesPrueba =  len(listaDescriptoresPrueba)
test_features = np.array([np.zeros(noClusters) for i in range(cantImagenesPrueba)])
print("Tamaño de test_features (imágenes con decriptores x número de Clústers):", test_features.shape)
for i in range(cantImagenesPrueba):
    cantDescriptoresImagen = 0 if listaDescriptoresPrueba[i] is None else len(listaDescriptoresPrueba[i])
    for j in range(cantDescriptoresImagen):
        feature = listaDescriptoresPrueba[i][j]
        feature = feature.reshape(1, 128)
        idx = kmeans.predict(feature)
        test_features[i][idx] += 1

test_features = scale.transform(test_features)

# Predicciones
kernel_test = test_features
predicciones = svm.predict(kernel_test)
print("Predicciones:\n", predicciones)

# Exactitud
from sklearn.metrics import accuracy_score
print ('Exactitud: %0.3f' % accuracy_score(labels_prueba, predicciones))

CSV_PRUEBA = "/content/drive/MyDrive/Colab Notebooks/resultados/Outex13_grises_50_prueba.csv"

with open(CSV_PRUEBA,'w+') as f:
    w = csv.writer(f)
    titulos = ["CLASE"] + list(range(noClusters))
    w.writerow(titulos)
    for i in range(len(test_features)):
      fila = []
      fila.append(labels_prueba[i])
      fila = fila + list(test_features[0])
      w.writerow(fila)