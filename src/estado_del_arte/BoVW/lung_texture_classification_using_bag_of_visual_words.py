# -*- coding: utf-8 -*-
"""Lung_Texture_Classification_Using_Bag_of_Visual_Words.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Swukdy5l0l_MwkYxNUolQ1ieYv_DH9S2

Implementación del paper "Lung Texture Classification Using Bag of Visual Words"

Importar los archivos de google drive
"""

from google.colab import drive
drive.mount('/content/gdrive')

import numpy as np
import cv2
from skimage.util import view_as_blocks
from skimage.util import view_as_windows
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from matplotlib import pyplot as plt
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score
from sklearn.svm import SVC
from sklearn.metrics import confusion_matrix
from sklearn.metrics import recall_score
from sklearn.metrics import precision_score
from sklearn.metrics import f1_score
from sklearn.metrics.pairwise import euclidean_distances

"""# **Fase de entrenamiento**

Se lee el archivo que tiene el nombre de cada imagen de entrenamiento junto a su clase.
"""

TRAIN_FILE = "/content/gdrive/My Drive/Outex-TC-00013/000/train.txt"
NUMBER_TRAIN_IMAGES = 680
NUMBER_IMAGES_PER_CATEGORY = 10
NUMBER_CLASSES = 68

train_filenames, train_real_labels = read_files(TRAIN_FILE, NUMBER_TRAIN_IMAGES)

print("Imagenes de entrenamiento:", train_filenames)
print("\n")
print("Clase a que pertenece cada imagen de entrenamiento:", train_real_labels)

"""### **1. Extracción de parches (obtención de features):**

1.   Se extraen parches cuadrados.
2.   Cada parche se convierte en vector (vendrían a ser los descriptores). 


"""

IMAGES_PATH = "/content/gdrive/My Drive/Outex-TC-00013/images/"
PATCH_SIZE = (7,7) # tamaño de parche
PATCH_STEP = 3

features_train_list = []

for img_filename in train_filenames:
  img = cv2.imread(IMAGES_PATH+img_filename, 0) # lee cada imagen de entranamiento
  patches = extract_patches(img, PATCH_SIZE, PATCH_STEP) # obtiene los feature vectors
  features_train_list.append(patches) # lista de vectores parches 

descriptors_train_x = vstackDescriptors(features_train_list) 
number_descriptors_per_image = len(features_train_list[0])

print("Cantidad de features por imagen:", number_descriptors_per_image,"\nDimensión de features:", len(features_train_list[0][0]))

"""### **2.   PCA:**
1. Se realiza PSA sobre los vectores del paso 1.
"""

NUMBER_COMPONENTS = 10 # componentes principales (tamaño que tendra cada palabra)

x = StandardScaler().fit_transform(descriptors_train_x) # Estandarizar los descriptores

pca = PCA(n_components = NUMBER_COMPONENTS)
descriptors_train = pca.fit_transform(x)

print("Número total de features:",descriptors_train.shape[0])
print("Dimensión de cada feature vector luego de aplicar PCA:",descriptors_train.shape[1])

"""### **3. Clusterización y creación de diccionario de textura:**

1. Por cada clase (categoría):
    1. Los vectores resultantes de PSA se clusterizan utilizando K-means.
    2. La salida del algoritmo K-means son los k vectores significativos (palabras visuales) que representa a cada categoría.
    3. Y se crea el diccionario de la categoría, con sus palabras visuales. Todos los diccionarios de cada categoría tienen el mismo número de palabras.
2. Luego, los diccionarios de todas las categorías se concentran en un solo diccionario global.
"""

number_descriptor_per_class = descriptors_train.shape[0]//NUMBER_CLASSES
descriptors_train_per_class = descriptors_train.reshape(NUMBER_CLASSES, number_descriptor_per_class, NUMBER_COMPONENTS)
NUMBER_CLUSTER = 10 # cada sub-diccionario (de cada clase) tendrá NUMBER_CLUSTER palabras

dictionary = []
for i in range(NUMBER_CLASSES):
  kmeans = cluster_descriptors(descriptors_train_per_class[i], NUMBER_CLUSTER)
  dictionary.append(kmeans.cluster_centers_)

print(len(dictionary)) # un sub-diccionario por categoría
print(len(dictionary[0])) # cantidad de palabras por sub-diccionarop
print(len(dictionary[0][0])) # tamaño de palabra

GLOBAL_DICTIONARY_LENGTH = NUMBER_CLASSES*NUMBER_CLUSTER 
global_dictionary = np.array(dictionary).reshape(GLOBAL_DICTIONARY_LENGTH,NUMBER_COMPONENTS)
global_dictionary.shape

"""### **Paso 4 y 5:**
**4. Cuantificación utilizando el diccionario de textura:**
1. Para cada imagen de entrenamiento:
  1. Cada vector se clasifica según la palabra visual más cercana en el diccionario. La cuantificación se basa en la distancia euclidiana.

**5. Creación de histograma de palabras de textura:**
1. Para cada imagen de entrenamiento:
  1. Para representar el ROI completo anotado, se genera un histograma de palabras de textura normalizada, utilizando el diccionario de textura global, para cada AROI de entrenamiento y prueba (como se muestra en la Figura 4). El histograma representa la distribución de palabras visuales en un AROI.
"""

print(descriptors_train.shape)
descriptors_per_image_train = descriptors_train.reshape(NUMBER_TRAIN_IMAGES, number_descriptors_per_image, NUMBER_COMPONENTS)
print(descriptors_per_image_train.shape)

histogram_per_image_train = create_histogram_per_image(global_dictionary,descriptors_per_image_train,NUMBER_TRAIN_IMAGES, NUMBER_CLUSTER, NUMBER_CLASSES)

"""Histograma de la imagen 0 (clase 0)"""

plot_histogram(histogram_per_image_train[0], GLOBAL_DICTIONARY_LENGTH, "Histograma imagen 0 (clase 0)")

"""Histograma de la imagen 10 (clase 1)"""

plot_histogram(histogram_per_image_train[10], GLOBAL_DICTIONARY_LENGTH, "Histograma imagen 10 (clase 1)")

"""Histograma de la imagen 20 (clase 3)"""

plot_histogram(histogram_per_image_train[20], GLOBAL_DICTIONARY_LENGTH, "Histograma imagen 20 (clase 2)")

"""Histograma de la imagen 30 (clase 4)"""

plot_histogram(histogram_per_image_train[30], GLOBAL_DICTIONARY_LENGTH, "Histograma imagen 30 (clase 3)")

"""Histograma de la imagen 40 (clase 5)"""

plot_histogram(histogram_per_image_train[40], GLOBAL_DICTIONARY_LENGTH, "Histograma imagen 40 (clase 4)")

"""### **6. Entrenar al clasificador SVM:**

"""

KERNEL = histogram_intersection_kernel
print(histogram_per_image_train.shape, train_real_labels.shape)
svm = findSVM(histogram_per_image_train, train_real_labels, KERNEL)

"""# **Fase de prueba**

Se lee el archivo que tiene el nombre de cada imagen de prueba junto a su clase.
"""

TEST_FILE = "/content/gdrive/My Drive/Outex-TC-00013/000/test.txt"
NUMBER_TEST_IMAGES = 680

test_filenames, test_real_labels = read_files(TEST_FILE, NUMBER_TEST_IMAGES)

print("Imagenes de test:", test_filenames)
print("\n")
print("Clase a que pertenece cada imagen de test:", test_real_labels)

"""### **1. Extracción de parches:**
1. Para la imagen de prueba:
    1. Se extraen parches cuadrados.
    2. Cada parche se convierte en vector.

"""

features_test_list = []

for img_filename in test_filenames:
  img = cv2.imread(IMAGES_PATH+img_filename, 0) # lee cada imagen de test
  patches = extract_patches(img, PATCH_SIZE, PATCH_STEP) # obtiene los feature vectors
  features_test_list.append(patches) # lista de vectores parches 

descriptors_test_x = vstackDescriptors(features_test_list)

print("Cantidad de features por imagen:", number_descriptors_per_image,"\nDimensión de features:", len(features_test_list[0][0]))

"""### **2.   PCA:**
1. Se realiza PSA sobre los vectores del paso 1.
"""

x = StandardScaler().fit_transform(descriptors_test_x) # Estandarizar los descriptores

descriptors_test = pca.fit_transform(x)

print("Número total de features:",descriptors_test.shape[0])
print("Dimensión de cada feature vector luego de aplicar PCA:",descriptors_test.shape[1])

"""### **Paso 3 y 4:**
**3. Cuantificación utilizando el diccionario de textura:**
1. Para cada imagen de entrenamiento:
  1. Cada vector se clasifica según la palabra visual más cercana en el diccionario. La cuantificación se basa en la distancia euclidiana.

**4. Creación de histograma de palabras de textura:**
1. Para cada imagen de entrenamiento:
  1. Para representar el ROI completo anotado, se genera un histograma de palabras de textura normalizada, utilizando el diccionario de textura global, para cada AROI de entrenamiento y prueba (como se muestra en la Figura 4). El histograma representa la distribución de palabras visuales en un AROI.
"""

print(descriptors_test.shape)
descriptors_per_image_test = descriptors_test.reshape(NUMBER_TEST_IMAGES, number_descriptors_per_image, NUMBER_COMPONENTS)
print(descriptors_per_image_test.shape)

histogram_per_image_test = create_histogram_per_image(global_dictionary,descriptors_per_image_test,NUMBER_TEST_IMAGES, NUMBER_CLUSTER, NUMBER_CLASSES)

"""### **5. Clasificación:**
Cada imagen se clasifica en una de una de las clases utilizando un clasificador de máquina de vectores de soporte (SVM):
"""

predictions = svm.predict(histogram_per_image_test)

predictions

"""**Accuracy**

accuracy = (TP + TN) / (TP + TN + FP + FN)
"""

accuracy = accuracy_score(test_real_labels, predictions)
print("accuracy:", accuracy)

"""**Recall**

recall = TP / (TP + FN)
"""

recall = recall_score(test_real_labels, predictions, average=None)
print("recall:", recall)

"""**Precision**

precision = TP / (TP + FP)
"""

precision = precision_score(test_real_labels, predictions, average=None, zero_division=1)
 print("precision:", precision)

"""**F-Score**

F1 = 2 * (precision * recall) / (precision + recall)
"""

f_score = f1_score(test_real_labels, predictions, average=None)
print("f-score:", f_score)

"""**Matriz de confusión**"""

matriz_confusion = confusion_matrix(test_real_labels, predictions)

print("matriz de confusión:\n", matriz_confusion)

for i in range(NUMBER_CLASSES):
  for j in range(NUMBER_CLASSES):
    value = matriz_confusion[i][j]
    if i==j and value == 10:
      print("La clase con 100% de acierto:", i)
    if i!=j and value != 0:
      print("Clase verdadera:",i,"clase predicha:",j, "número de veces:", value )

plot_confusions(test_real_labels, predictions, NUMBER_CLASSES)



"""# **Funciones**"""

def read_files(path,number_images):
  f = open(path,"r")
  lines = np.array([l.split() for l in f.readlines()[1:number_images+1]])
  
  filenames = lines[:,0]  # array con los nombres de archivo de las imagenes
  real_labels = lines[:,1] # array con la clase a que pertenece cada imagen
  # donde real_labels[i] corresponde a la clase que pertenece la imagen filenames[i]
  
  return filenames, real_labels

def extract_patches(image, patch_size, step):
  view = view_as_windows(image, patch_size, step) # crea ventanas cuadradas 

  return view.reshape(-1,patch_size[0]*patch_size[1]) # convierte cada ventana en un vector

def vstackDescriptors(descriptor_list):
  descriptors = np.array(descriptor_list[0])
  for descriptor in descriptor_list[1:]:
      descriptors = np.vstack((descriptors, descriptor))  # apenda matrices con la misma cantidad de columnas en una sola matriz
  return descriptors

def cluster_descriptors(descriptors, no_clusters):
  kmeans = KMeans(n_clusters = no_clusters, n_jobs=-1).fit(descriptors)
  return kmeans

def create_histogram_per_image(global_dictionary, descriptor_list, image_count, no_clusters, no_classes):

  histogram_per_image = np.array([np.zeros(no_clusters*no_classes) for i in range(image_count)])  # array con NUMERO DE IMAGENES x no_clusters

  for i in range(image_count): #  por cada imagen
    for j in range(len(descriptor_list[i])): # cada descriptor de la imagen i
      feature = descriptor_list[i][j]
      distances = euclidean_distances([feature], global_dictionary) # encuentra la palabra más cercana en el diccionario de palabras
      index_min = np.argmin(distances) # obtiene su indice
      histogram_per_image[i][index_min] += 1 # crea histograma por imagen
    histogram_per_image[i] = histogram_per_image[i]/sum(histogram_per_image[i]) # normalizar histograma

  return histogram_per_image

def findSVM(features, train_labels, kernel):
  svm = SVC(kernel = kernel)
  svm.fit(features, train_labels)
  return svm

# calculate the kernel matrix for x and  examples 
# kernel is (n_samples x 1 ) matrix
# n_samples number of the examples
def histogram_intersection_kernel(data_1, data_2):
  kernel = np.zeros((data_1.shape[0], data_2.shape[0]))

  for d in range(data_1.shape[1]):
    column_1 = data_1[:, d].reshape(-1, 1)
    column_2 = data_2[:, d].reshape(-1, 1)
    kernel += np.minimum(column_1, column_2.T)

  return kernel

def plot_histogram(histogram, no_clusters, title):
    x_scalar = np.arange(no_clusters)
    y_scalar = np.array(histogram)
    # suma todos los valores de la columna i y guarda la suma en y_scalar[i]

    plt.bar(x_scalar, y_scalar)
    plt.xlabel("Visual Word Index")
    plt.ylabel("Frequency")
    plt.title(title)
    plt.xticks(x_scalar + 0.4, x_scalar)
    plt.show()

def plot_confusions(true, predictions, no_classes):
    np.set_printoptions(precision=2)

    class_names = np.arange(no_classes)
    plotConfusionMatrix(true, predictions, classes=class_names,
                      title='Confusion matrix, without normalization')

    plotConfusionMatrix(true, predictions, classes=class_names, normalize=True,
                      title='Normalized confusion matrix')

    plt.show()

def plotConfusionMatrix(y_true, y_pred, classes,
                          normalize=False,
                          title=None,
                          cmap=plt.cm.Blues):
    if not title:
        if normalize:
            title = 'Normalized confusion matrix'
        else:
            title = 'Confusion matrix, without normalization'

    cm = confusion_matrix(y_true, y_pred)
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        print("Normalized confusion matrix")
    else:
        print('Confusion matrix, without normalization')

    print(cm)

    fig, ax = plt.subplots()
    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)
    ax.figure.colorbar(im, ax=ax)
    ax.set(xticks=np.arange(cm.shape[1]),
           yticks=np.arange(cm.shape[0]),
           xticklabels=classes, yticklabels=classes,
           title=title,
           ylabel='True label',
           xlabel='Predicted label')

    plt.setp(ax.get_xticklabels(), rotation=45, ha="right",
             rotation_mode="anchor")

    fmt = '.2f' if normalize else 'd'
    thresh = cm.max() / 2.
    for i in range(cm.shape[0]):
        for j in range(cm.shape[1]):
            ax.text(j, i, format(cm[i, j], fmt),
                    ha="center", va="center",
                    color="white" if cm[i, j] > thresh else "black")
    fig.tight_layout()
    return ax

a = np.asarray([ [1,2,3], [4,5,6], [7,8,9] ])
np.savetxt("foo.csv", a, delimiter=",",fmt='%s')