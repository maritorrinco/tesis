# -*- coding: utf-8 -*-
"""BoVW con SIFT - 28 10.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1czvvfa10-teLcGy3C2j5SRe5B-7OBT_l

#### **Instalación SIFT**
"""

!pip install opencv-python==3.4.2.16
!pip install opencv-contrib-python==3.4.2.16

"""# **Entrenamiento**

## **Aplicación de SIFT a imágenes de entrenamiento**
"""

import cv2
import glob
import numpy as np 

# Lectura de imágenes de entrenamiento del archivo train.txt
f = open("/content/drive/My Drive/Colab Notebooks/Outex-TC-00013-lum/000/train.txt","r")
lineas = f.readlines()

listaDescriptores = []
labels_entrenamiento = np.array([])

cantImagenes = 0

# Extracción de keypoints / descriptores de las imágenes de entrenamiento utilizando SIFT
sift = cv2.xfeatures2d_SIFT.create()
for i in range(1, len(lineas)):
    rutaImagen = "/content/drive/My Drive/Colab Notebooks/Outex-TC-00013-lum/images/" + lineas[i].split()[0]
    imagen = cv2.imread("/content/drive/My Drive/Colab Notebooks/Outex-TC-00013-lum/images/" + lineas[i].split()[0])

    escalaGrises = cv2.cvtColor(imagen, cv2.COLOR_BGR2GRAY)
    keypoints, des = sift.detectAndCompute(escalaGrises, None)
    
    if(des is not None):
      labels_entrenamiento = np.append(labels_entrenamiento, lineas[i].split()[1])
      listaDescriptores.append(des)
      cantImagenes = cantImagenes + 1

print('Cantidad de imágenes con descriptores', len(listaDescriptores))
print('Clases de cada imagen ', labels_entrenamiento)

"""## **K-means**

### **Convertir la lista de descriptores a np.array**
"""

descriptores = np.array(listaDescriptores[0])
for descriptor in listaDescriptores[1:]:
  descriptores = np.vstack((descriptores, descriptor)) 

descriptores

"""### **Generar K-means con los keypoints obtenidos**"""

from sklearn.cluster import KMeans
noClusters = 10
kmeans = KMeans(init="random", n_clusters = noClusters, n_init=10, max_iter=300, random_state=None).fit(descriptores)

# max_iter: Número máximo de iteraciones del algoritmo k-means para una sola ejecución. Default = 300
# n_init: Número de veces que se ejecutará el algoritmo de k-means con diferentes semillas de centroide.
#         Los resultados finales serán la mejor salida de n_init corridas consecutivas en términos de inercia. Default = 10

# Extraer keypoints / características de cada imagen
im_features = np.array([np.zeros(noClusters) for i in range(cantImagenes)])
print("Tamaño de im_features (imágenes con decriptores x número de Clústers):", im_features.shape)
for i in range(cantImagenes):
    for j in range(len(listaDescriptores[i])):
        feature = listaDescriptores[i][j]
        feature = feature.reshape(1, 128)
        # A qué cluster pertenece
        idx = kmeans.predict(feature)
        im_features[i][idx] += 1

print("Ejemplo de cantidad de palabras visuales en una imagen (histograma):", im_features[0])

from sklearn.preprocessing import StandardScaler

# Normalizar características
scale = StandardScaler().fit(im_features)        
im_features = scale.transform(im_features)

# Histograma del vocabulario obtenido

from matplotlib import pyplot as plt
# Plot
x_scalar = np.arange(noClusters)
y_scalar = np.array([abs(np.sum(im_features[:,h], dtype=np.int32)) for h in range(noClusters)])

plt.bar(x_scalar, y_scalar)
plt.xlabel("Índice de Palabras Visuales")
plt.ylabel("Frecuencia")
plt.title("Vocabulario obtenido")
plt.xticks(x_scalar + 0.4, x_scalar)
plt.show()

print("Cantidad de veces que aparece la P0 en las imágenes de entrenamiento =", np.sum(im_features[:,0], dtype=np.int32))
print("Cantidad de veces que aparece la P1 en las imágenes de entrenamiento =", np.sum(im_features[:,1], dtype=np.int32))
print("Cantidad de veces que aparece la P2 en las imágenes de entrenamiento =", np.sum(im_features[:,2], dtype=np.int32))
print("Cantidad de veces que aparece la P3 en las imágenes de entrenamiento =", np.sum(im_features[:,3], dtype=np.int32))
print("Cantidad de veces que aparece la P4 en las imágenes de entrenamiento =", np.sum(im_features[:,4], dtype=np.int32))
print("Cantidad de veces que aparece la P5 en las imágenes de entrenamiento =", np.sum(im_features[:,5], dtype=np.int32))
print("Cantidad de veces que aparece la P6 en las imágenes de entrenamiento =", np.sum(im_features[:,6], dtype=np.int32))
print("Cantidad de veces que aparece la P7 en las imágenes de entrenamiento =", np.sum(im_features[:,7], dtype=np.int32))
print("Cantidad de veces que aparece la P8 en las imágenes de entrenamiento =", np.sum(im_features[:,8], dtype=np.int32))
print("Cantidad de veces que aparece la P9 en las imágenes de entrenamiento =", np.sum(im_features[:,9], dtype=np.int32))

"""## **Entrenamiento del Clasificador SVM**"""

from sklearn.svm import SVC
from sklearn.multiclass import OneVsRestClassifier

svm = OneVsRestClassifier(SVC(kernel = 'linear'))
svm.fit(im_features, labels_entrenamiento)

"""# **Prueba**

## **Extracción de keypoints a imágenes de prueba con SIFT**
"""

# Lectura de prueba de prueba del archivo test.txt
f = open("/content/drive/My Drive/Colab Notebooks/Outex-TC-00013-lum/000/test.txt","r")
lineas = f.readlines()

# Extracción de keypoints de las imágenes de prueba utilizando SIFT
count = 0
true = []
listaDescriptores = []
name_dict =	{}

sift = cv2.xfeatures2d_SIFT.create()
for i in range(1, len(lineas)):
    rutaImagen = "/content/drive/My Drive/Colab Notebooks/Outex-TC-00013-lum/images/" + lineas[i].split()[0]
    imagen = cv2.imread(rutaImagen)

    escalaGrises = cv2.cvtColor(imagen, cv2.COLOR_BGR2GRAY)
    keypoints, des = sift.detectAndCompute(escalaGrises, None)
    if(des is not None):
      count += 1
      listaDescriptores.append(des)
      true.append(lineas[i].split()[1])

print('Cantidad de imágenes de prueba con descriptores', len(listaDescriptores))
print('Clases de cada imagen ', true)
print(len(true))

"""## **K-means**

### **Asignar cada descriptor a un Clúster del K-means**
"""

test_features = np.array([np.zeros(noClusters) for i in range(count)])
print("Tamaño de test_features (imágenes con decriptores x número de Clústers):", test_features.shape)
for i in range(count):
    for j in range(len(listaDescriptores[i])):
        feature = listaDescriptores[i][j]
        feature = feature.reshape(1, 128)
        idx = kmeans.predict(feature)
        test_features[i][idx] += 1

test_features = scale.transform(test_features)

"""## **Predicciones utilizando SVM**"""

kernel_test = test_features
predicciones = svm.predict(kernel_test)
print("Predicciones:\n", predicciones)

"""## **Resultados**

### **Matriz de Confusión**
"""

from sklearn.metrics import confusion_matrix
def matrizDeConfusion(y_true, y_pred, normalizada=False):

    cm = confusion_matrix(y_true, y_pred)
    if normalizada:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        print("Matriz de Confusión Normalizada")
    else:
        print('Matriz de Confusión sin Normalizar')

    print("Tamaño de la matriz de confusión: ", cm.shape)
    print(cm)

    print("...primeras cinco clases:")
    print(cm[:5, :])

np.set_printoptions(precision=2)

from collections import Counter
print('Diferentes clases en los valores reales:', len(Counter(true)))
print(Counter(true).most_common())

print()

print("La razón por la cuál no son las 68 clases probablemente sea que algunas clases pudieron ser descartadas al no obtenerse keypoints con SIFT.")

print()

print('Diferentes clases en los valores predichos:', len(Counter(predicciones)))
print(Counter(predicciones).most_common())

print('\n\n')

matrizDeConfusion(true, predicciones)

print()

matrizDeConfusion(true, predicciones, normalizada=True)

"""### **Exactitud**"""

from sklearn.metrics import accuracy_score
print ('Exactitud: %0.3f' % accuracy_score(true, predicciones))

"""Exactitudes obtenidas de acuerdo a diferentes tamaños de vocabulario (número de clústers):


*   10 -> 0.176 -> 0.406
*   50 -> 0.475
*   100 -> 0.378
*   250 -> 0.441
*   400 -> 0.426
*   500 -> 0.404
*   750 -> 0.400
*   1000 -> 0.351


"""