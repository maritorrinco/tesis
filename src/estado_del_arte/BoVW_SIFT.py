# -*- coding: utf-8 -*-
"""BoVW con SIFT - 28 10.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1czvvfa10-teLcGy3C2j5SRe5B-7OBT_l

#### **Instalación SIFT**
"""
#!pip3 install -U opencv-contrib-python==3.4.2.16

"""# **Entrenamiento**
## **Aplicación de SIFT a imágenes de entrenamiento**
"""

import sys
import cv2
import glob
import numpy as np 
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.multiclass import OneVsRestClassifier
#from matplotlib import pyplot as plt
from sklearn.metrics import confusion_matrix
from collections import Counter
from sklearn.metrics import accuracy_score
from sklearn.svm import LinearSVC
import csv
from datetime import datetime
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import MultinomialNB
from sklearn.neural_network import MLPClassifier

# Parametros bash:
# Nombre de la base de datos en el cluster
base_datos_param = sys.argv[1]
print("Base de datos parametro:", base_datos_param)
# Clasificador
CLASSIFIER = sys.argv[2]
print("Clasificador parametro:", CLASSIFIER)
# random_state (de KMEANS)
RANDOM_STATE = int(sys.argv[3])

# Funciones
def read_images(txt): # txt: archivo.txt
  labels = np.array([])

  # Lectura de nombres de archivos de entrenamiento
  path_base = PATH_BASE
  f = open(path_base + "000/" + txt,"r")
  lineas = f.readlines()

  imagenes = []
  for i in range(1, len(lineas)):
    nombreArchivo = lineas[i].split()[0]
    labels = np.append(labels, lineas[i].split()[1])
    path = path_base + "images/" + nombreArchivo
    img = cv2.imread(path)
    imagenes.append(img)
  return imagenes, labels # retorna un array de imágenes y un array de nombre de clases

PATH_BASE = "../databases/" + base_datos_param + "/"

# Lectura imagenes entrenamiento
print("Lectura de imágenes de entrenamiento...")
imagenes_entrenamiento, labels_entrenamiento =  read_images("train.txt")

# Lectura de imágnes de prueba
print("Lectura de imágenes de prueba...")
imagenes_prueba, labels_prueba =  read_images("test.txt")

print("Entrenamiento...")
# Extracción de keypoints / descriptores de las imágenes de entrenamiento utilizando SIFT
print("Extraccion de keypoints con SIFT de las imagenes de entrenamiento...")
listaDescriptoresEntrenamiento = []
labels_entrenamiento_sift = []
sift = cv2.xfeatures2d_SIFT.create()
for i in range(len(imagenes_entrenamiento)):
    imagen = imagenes_entrenamiento[i]
    escalaGrises = cv2.cvtColor(imagen, cv2.COLOR_BGR2GRAY)
    keypoints, des = sift.detectAndCompute(escalaGrises, None)
    
    if(des is not None):
      #print('des...', des)
      listaDescriptoresEntrenamiento.append(des)
      labels_entrenamiento_sift.append(labels_entrenamiento[i])
      #cantImagenes = cantImagenes + 1

print('Cantidad de imagenes de entrenamiento', len(imagenes_entrenamiento))
print('Cantidad de imágenes con descriptores:', len(listaDescriptoresEntrenamiento))
print('Ejemplo cantidad de descriptores de una imagen', len(listaDescriptoresEntrenamiento[0]))
print('Ejemplo descriptores de una imagen', listaDescriptoresEntrenamiento[0])
#print('Cantidad de imágenes con descriptores', len(listaDescriptores))
#print('Clases de cada imagen ', labels_entrenamiento)

"""## **K-means**
### **Convertir la lista de descriptores a np.array**
"""

descriptores = np.array(listaDescriptoresEntrenamiento[0])
for descriptor in listaDescriptoresEntrenamiento[1:]:
  descriptores = np.vstack((descriptores, descriptor)) 


#descriptores

"""### **Generar K-means con los keypoints obtenidos**"""
print("K-means...")
noClusters = 50
#kmeans = KMeans(init="random", n_clusters = noClusters, n_init=10, max_iter=300, random_state=None).fit(descriptores)
kmeans = KMeans(init="random", n_clusters = noClusters, n_init=10, max_iter=300, random_state=RANDOM_STATE).fit(descriptores)

# max_iter: Número máximo de iteraciones del algoritmo k-means para una sola ejecución. Default = 300
# n_init: Número de veces que se ejecutará el algoritmo de k-means con diferentes semillas de centroide. Default = 10
#         Los resultados finales serán la mejor salida de n_init corridas consecutivas en términos de inercia. Default = 10

# Extraer keypoints / características de cada imagen
print("Prueba...")
cantImagenes = len(listaDescriptoresEntrenamiento)
im_features = np.array([np.zeros(noClusters) for i in range(cantImagenes)])
print("Tamaño de im_features (imágenes con decriptores x número de Clústers):", im_features.shape)
for i in range(cantImagenes):
    for j in range(len(listaDescriptoresEntrenamiento[i])):
        feature = listaDescriptoresEntrenamiento[i][j]
        feature = feature.reshape(1, 128)
        # A qué cluster pertenece
        idx = kmeans.predict(feature)
        im_features[i][idx] += 1

print("Ejemplo de cantidad de palabras visuales en una imagen (histograma):", im_features[0])

# Normalizar características
scale = StandardScaler().fit(im_features)        
im_features = scale.transform(im_features)

# Histograma del vocabulario obtenido

# Plot
#x_scalar = np.arange(noClusters)
#y_scalar = np.array([abs(np.sum(im_features[:,h], dtype=np.int32)) for h in range(noClusters)])

#plt.bar(x_scalar, y_scalar)
#plt.xlabel("Índice de Palabras Visuales")
#plt.ylabel("Frecuencia")
#plt.title("Vocabulario obtenido")
#plt.xticks(x_scalar + 0.4, x_scalar)
#plt.show()

#print("Cantidad de veces que aparece la P0 en las imágenes de entrenamiento =", np.sum(im_features[:,0], dtype=np.int32))
#print("Cantidad de veces que aparece la P1 en las imágenes de entrenamiento =", np.sum(im_features[:,1], dtype=np.int32))
#print("Cantidad de veces que aparece la P2 en las imágenes de entrenamiento =", np.sum(im_features[:,2], dtype=np.int32))
#print("Cantidad de veces que aparece la P3 en las imágenes de entrenamiento =", np.sum(im_features[:,3], dtype=np.int32))
#print("Cantidad de veces que aparece la P4 en las imágenes de entrenamiento =", np.sum(im_features[:,4], dtype=np.int32))
#print("Cantidad de veces que aparece la P5 en las imágenes de entrenamiento =", np.sum(im_features[:,5], dtype=np.int32))
#print("Cantidad de veces que aparece la P6 en las imágenes de entrenamiento =", np.sum(im_features[:,6], dtype=np.int32))
#print("Cantidad de veces que aparece la P7 en las imágenes de entrenamiento =", np.sum(im_features[:,7], dtype=np.int32))
#print("Cantidad de veces que aparece la P8 en las imágenes de entrenamiento =", np.sum(im_features[:,8], dtype=np.int32))
#print("Cantidad de veces que aparece la P9 en las imágenes de entrenamiento =", np.sum(im_features[:,9], dtype=np.int32))

"""## **Entrenamiento del Clasificador SVM**"""
print("shape de im_features en el clasficador",  im_features.shape)
print("primer elemento de im_features en el clasificador", im_features[0])
#svm = OneVsRestClassifier(SVC(kernel = 'linear'))
#svm = LinearSVC()
#svm.fit(im_features, labels_entrenamiento_sift)

print("Entrenando clasificador...")
if CLASSIFIER == 'ovsr':
  svm = OneVsRestClassifier(SVC(kernel = 'linear'))
  svm.fit(im_features, labels_entrenamiento_sift)
  _classifier = svm
elif CLASSIFIER == 'svc':
  svm = SVC(kernel = 'linear')
  svm.fit(im_features, labels_entrenamiento_sift)
  _classifier = svm
elif CLASSIFIER == 'knn':
  n_neighbors = 1
  knn = KNeighborsClassifier(n_neighbors)
  knn.fit(im_features, labels_entrenamiento_sift)
  _classifier = knn
elif CLASSIFIER == 'rfc':
  rfc = RandomForestClassifier(random_state=0)
  rfc.fit(im_features, labels_entrenamiento_sift)
  _classifier = rfc
elif CLASSIFIER == 'mnb':
  model = MultinomialNB()
  model.fit(im_features, labels_entrenamiento_sift)
  _classifier = model
elif CLASSIFIER == 'mlp':
  clf = MLPClassifier(random_state=0, max_iter=700).fit(im_features, labels_entrenamiento_sift)
  _classifier = clf

"""# **Prueba**

## **Extracción de keypoints a imágenes de prueba con SIFT**
"""

# Extracción de keypoints de las imágenes de prueba utilizando SIFT
#count = 0
true = []
listaDescriptoresPrueba = []
name_dict =	{}

#for imagen in imagenes_prueba:
for i in range(len(imagenes_prueba)):
    imagen = imagenes_prueba[i]
    escalaGrises = cv2.cvtColor(imagen, cv2.COLOR_BGR2GRAY)
    keypoints, des = sift.detectAndCompute(escalaGrises, None)
    if(des is not None):
      #count += 1
      listaDescriptoresPrueba.append(des)
      #true.append(lineas[i].split()[1])
      true.append(labels_prueba[i])

print('Cantidad de imágenes de prueba', len(imagenes_prueba))
print('Cantidad de imagenes con descriptores', len(listaDescriptoresPrueba))
#print('Clases de cada imagen ', true)
#print(len(true))

"""## **K-means**

### **Asignar cada descriptor a un Clúster del K-means**
"""

count = len(listaDescriptoresPrueba)
test_features = np.array([np.zeros(noClusters) for i in range(count)])
print("Tamaño de test_features (imágenes con decriptores x número de Clústers):", test_features.shape)
for i in range(count):
    for j in range(len(listaDescriptoresPrueba[i])):
        feature = listaDescriptoresPrueba[i][j]
        feature = feature.reshape(1, 128)
        idx = kmeans.predict(feature)
        test_features[i][idx] += 1

test_features = scale.transform(test_features)

"""## **Predicciones utilizando SVM**"""

kernel_test = test_features
#predicciones = svm.predict(kernel_test)
predicciones = _classifier.predict(kernel_test)
print("Predicciones:\n", predicciones)

"""## **Resultados**
### **Matriz de Confusión**
"""
def matrizDeConfusion(y_true, y_pred, normalizada=False):
    cm = confusion_matrix(y_true, y_pred)
    if normalizada:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        print("Matriz de Confusión Normalizada")
    else:
        print('Matriz de Confusión sin Normalizar')

    print("Tamaño de la matriz de confusión: ", cm.shape)
    print(cm)

    print("...primeras cinco clases:")
    print(cm[:5, :])

#np.set_printoptions(precision=2)

#print('Diferentes clases en los valores reales:', len(Counter(true)))
#print(Counter(true).most_common())

#print("La razón por la cuál no son las 68 clases probablemente sea que algunas clases pudieron ser descartadas al no obtenerse keypoints con SIFT.")

#print('Diferentes clases en los valores predichos:', len(Counter(predicciones)))
#print(Counter(predicciones).most_common())

#matrizDeConfusion(true, predicciones)
#print()
#matrizDeConfusion(true, predicciones, normalizada=True)

"""### **Exactitud**"""
print ('Exactitud:', accuracy_score(true, predicciones))

print("Guardar resultados en archivos...")
now = datetime.now()
FECHA_HORA = str(now.year) + '-' + str(now.month) + '-' + str(now.day) + '-' + str(now.hour)  + str(now.minute) + str(now.second)
CSV_PATH = "../resultados/BoVW_SIFT_RANDOM" + str(RANDOM_STATE) + "_" + str(noClusters) + "_" + base_datos_param + '_' + FECHA_HORA +  "_" + CLASSIFIER + ".csv"

titulos = ["Exactitud"] # Para imprimir en csv
with open(CSV_PATH,'w') as f:
  w = csv.writer(f)
  w.writerow(titulos)
  w.writerow([accuracy_score(true, predicciones)])

# guardar vectores de caracteristicas
TRAIN_PATH = "../resultados/BoVW_SIFT_RANDOM" + str(RANDOM_STATE) + "_" + str(noClusters) + "_" + base_datos_param + "_train_" + FECHA_HORA + "_" + CLASSIFIER + ".csv"
TEST_PATH = "../resultados/BoVW_SIFT_RANDOM" + str(RANDOM_STATE) + "_" + str(noClusters) + "_" + base_datos_param + "_test_" + FECHA_HORA + "_" + CLASSIFIER + ".csv"

with open(TRAIN_PATH,'w+') as f:
  w = csv.writer(f)
  titulos = ["CLASE"] + list(range(im_features.shape[1]))
  w.writerow(titulos)
  for i in range(im_features.shape[0]):
    fila = []
    fila.append(labels_entrenamiento_sift[i])
    fila = fila + list(im_features[i])
    w.writerow(fila)

with open(TEST_PATH,'w+') as f:
  w = csv.writer(f)
  titulos = ["CLASE"] + list(range(test_features.shape[1]))
  w.writerow(titulos)
  for i in range(test_features.shape[0]):
    fila = []
    fila.append(true[i])
    fila = fila + list(test_features[i])
    w.writerow(fila)

print("Terminado!!")

"""Exactitudes obtenidas de acuerdo a diferentes tamaños de vocabulario para Outex13 (número de clústers):

*   10 -> 0.176 -> 0.406
*   50 -> 0.475
*   100 -> 0.378
*   250 -> 0.441
*   400 -> 0.426
*   500 -> 0.404
*   750 -> 0.400
*   1000 -> 0.351

"""