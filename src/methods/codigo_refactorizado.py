# -*- coding: utf-8 -*-
"""VECTOR DE CARACTERISTICAS A CSV Código refactorizado.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IZoVF__rOCB_qRrjLvzo-f1hFTDzQriA

Orden de los parámetros bash:
base_de_datos metodo rango v1 v2 clasificador
"""

#from google.colab import drive
#drive.mount('/content/drive')

# Librerías
import sys
from skimage.util import view_as_windows
import cv2
from sklearn.feature_extraction.text import CountVectorizer
import numpy as np
from sklearn.svm import SVC
from sklearn.multiclass import OneVsRestClassifier
from sklearn.metrics import accuracy_score
from itertools import product
from termcolor import colored
from sklearn.metrics import confusion_matrix
import csv
import math
from scipy import stats as st
import statistics as stat
import itertools
import operator
from scipy.sparse import csr_matrix
from datetime import datetime
from sklearn.neighbors import KNeighborsClassifier
import scipy.sparse

# PARAMS
RANGES = [11]
# Parametro desde linea de comandos:
if len(sys.argv) > 3:
  RANGES = [int(sys.argv[3])]
  print("Rango parametro:", sys.argv[3])

STEPS = [1]

WINDOWS = [ [(4, 1)] ]
#WINDOWS = None
# Parametros desde linea de comandos:
if len(sys.argv) > 5:
  WINDOWS = [ [ (int(sys.argv[4]), int(sys.argv[5]) )] ]
  print("Dimension parametro:", int(sys.argv[4]) ,'x',  int(sys.argv[5]) )

# Dimensiones mínima y máxima (para filas y columnas) de las ventanas
MIN_DIMENSION = 1
MAX_DIMENSION = 4
INCREASE_BY = 2
# Cantidad mínima de tamaños de ventanas diferentes
MIN_COMBINED_WINDOWS = 1
# Cantidad máxima de tamañosd de ventanas diferentes
MAX_COMBINED_WINDOWS = 2

PATH_BASE = "../databases/Curet_modificado/"
# Outex 13: drive/My Drive/Colab Notebooks/Outex_TC_00013/
# Outex 14: drive/My Drive/Colab Notebooks/Outex_TC_00014/
# Brodatz: drive/My Drive/Colab Notebooks/Brodatz_dividido/
# Vistex: drive/My Drive/Colab Notebooks/Vistex_dividido/
# Curet: drive/My Drive/Colab Notebooks/Curet_modificado/
if len(sys.argv) > 1:
  dbase = sys.argv[1]
  print("Base de datos parametro:", dbase)
  PATH_BASE = "../databases/" + dbase + "/"

METHOD = 2
# Parametro desde linea de comandos:
if len(sys.argv) > 2:
  METHOD = int(sys.argv[2])
  print("Metodo parametro:", METHOD)
# 1: R+G+B
# 2: Concatenar caracter no usado

CLASSIFIER = 'ovsr'
# Posibles valores: ovsr; svc; knn
# Parametro desde linea de comandos:
if len(sys.argv) > 6:
  CLASSIFIER = sys.argv[6]
  print("Clasificador parametro:", CLASSIFIER)

FORMAT_VECT_CAR = 'npz'
# Posibles valores: csv; npz
# Parametro desde linea de comandos:
if len(sys.argv) > 7:
  FORMAT_VECT_CAR = sys.argv[7]
  print("Formato para los vectores de caracteristicas parametro:", FORMAT_VECT_CAR)

# Pre-procesamientos
PREPROCESSING = 0
# 0: None
# 1: HE_HSV
# 2: TOP_HAT_HSV
# 3: HE_RGB
# 4: TOP_HAT_RGB
# 5: HE_HSL
# 6: TOP_HAT_HSL

WITH_ROTATION = False

# Valores añadidos al vector de configuración:
# Por cada diccionario, crear una CONST SUM

QUANTITIES = {
    'RGB': False,
    'R': False,
    'G': False,
    'B': False
}

LEXICAL_DIVERSITY = {
    'TTR': False,
    'HERDAN': False,
    'SOMERS': False,
    'HONORE': False,
    'MAAS': False,
    'DUGAST': False
}

STATISTICS = {
    'ENTROPY': False,
    'STD': False, #standard deviation
    'MEAN': False,
    'MEDIAN': False,
    'MODE': False,
    'VARIANCE': False,
    'MAD': False, 
    'RANGE': False
}

NORMALIZED_EUCLIDEAN_DISTANCE = {
  'RED_GREEN': False,
  'RED_BLUE': False,
  'GREEN_BLUE': False,  
}

CANBERRA_DISTANCE = {
  'BLUE_RGB': False,
  'GREEN_RGB': False,
  'RED_RGB': False,
  'RED_GREEN': False,
  'RED_BLUE': False,
  'GREEN_BLUE': False,  
}

RATES = {
  'BLUE_RGB': False,
  'GREEN_RGB': False,
  'RED_RGB': False,
  'RED_GREEN': False,
  'RED_BLUE': False,
  'GREEN_RED': False,
  'GREEN_BLUE': False,
  'BLUE_RED': False,
  'BLUE_GREEN': False
}

# Resultados en:
now = datetime.now()
FECHA_HORA = str(now.year) + '-' + str(now.month) + '-' + str(now.day) + '-' + str(now.hour)  + str(now.minute) + str(now.second)
CSV_PATH = "../resultados/" + str(METHOD) + "_" + dbase + "_" + FECHA_HORA + "_" + CLASSIFIER + ".csv"
CSV_PRUEBA = "../resultados/vector_prueba_" + str(METHOD) + "_" + dbase + "_" + FECHA_HORA + "_" + CLASSIFIER
CSV_ENTRENAMIENTO = "../resultados/vector_entrenamiento_" + str(METHOD) + "_" + dbase + "_" + FECHA_HORA + "_" + CLASSIFIER
if FORMAT_VECT_CAR == 'csv':
  CSV_PRUEBA = CSV_PRUEBA + '.csv'
  CSV_ENTRENAMIENTO = CSV_ENTRENAMIENTO + '.csv'

# CONSTANTS
ALPHA = chr(0x03B1) # α - red
BETA = chr(0x03B2) # β - green
GAMMA = chr(0x0263) # γ - blue

# 3x3 indices:
# 0,0  0,1  0,2
# 1,0  1,1  1,2
# 2,0  2,1  2,2
NEIGHBORS_LIST = [(0,0), (0,1), (0,2), (1,2), (2,2), (2,1), (2,0), (1,0)]

SUM_QUANTITIES = sum(QUANTITIES[k] for k in QUANTITIES)
SUM_LEXICAL_DIVERSITY = sum(LEXICAL_DIVERSITY[k] for k in LEXICAL_DIVERSITY)
SUM_STATISTICS= sum(STATISTICS[k] for k in STATISTICS)
SUM_RATES = sum(RATES[k] for k in RATES)
SUM_NORMALIZED_EUCLIDEAN_DISTANCE = sum(NORMALIZED_EUCLIDEAN_DISTANCE[k] for k in NORMALIZED_EUCLIDEAN_DISTANCE)
SUM_CANBERRA_DISTANCE = sum(CANBERRA_DISTANCE[k] for k in CANBERRA_DISTANCE)

# Funtions
def generate_configurations():
  try:

    if WINDOWS is None and (MIN_DIMENSION > 0 and MAX_DIMENSION > 0 and INCREASE_BY > 0 and MIN_COMBINED_WINDOWS > 0 and MAX_COMBINED_WINDOWS > 0):
      dim_ventanas = {
        'r': list(range(MIN_DIMENSION, MAX_DIMENSION + 1, INCREASE_BY)),
        'c': list(range(MIN_DIMENSION, MAX_DIMENSION + 1, INCREASE_BY))
      }

      dim_ventanas_names, dim_ventanas_values = zip(*dim_ventanas.items())
      # Extracción de combinaciones de Ventanas
      tam_ventanas = [dict(zip(dim_ventanas_names, h)) for h in product(*dim_ventanas_values)]

      win_combinations = []

      for L in range(MIN_COMBINED_WINDOWS, MAX_COMBINED_WINDOWS + 1):
          for subconjunto in itertools.combinations(tam_ventanas, L):
            win_combinations.append(subconjunto)

      windows = []
      for comb in win_combinations:
        comb_array = []
        for w in comb:
          row = w.get("r")
          col = w.get("c")
          comb_array.append((row, col))
        windows.append(comb_array)

    elif WINDOWS is not None:
      windows = WINDOWS
    
    else:
      print("Verify params: all params should be > 0.")
      return None
    
    parametros = {'range': RANGES,
                  'windows': windows,
                  'step': STEPS}
    param_names, param_values = zip(*parametros.items())
    params_set = [dict(zip(param_names, h)) for h in product(*param_values)]

    return params_set

  except Exception as e:
    print("Verify params:", e)

def generate_characters():
  characters = ""

  start = 0x0021 # desde !
  end = 0x007E # hasta ~
  for i in range(start, end + 1):
    characters = characters+chr(i)

  start_1 = 0x00C0 # desde À
  end_1 = 0x00FF # hasta ÿ
  for i in range(start_1, end_1 + 1):
    characters = characters+chr(i)

  start_2 = 0x0100 # Ā
  end_2 = 0x0161 # š
  for i in range(start_2, end_2 + 1):
    characters = characters+chr(i)

  return characters

def get_alphabet_size(letras, _range):
  p1 = 0
  p2 = 0
  valoresPorLetra = {} # Para mostrar
  for i in range(len(letras)):
    p2 = p1 + (_range - 1)
    if p2 > 255:
      p2 = 255 
    valoresPorLetra[letras[i]] = list(range(p1, p2 + 1))
    p1 = p1 + _range

  tamAlfabeto = 0
  for i in range(len(valoresPorLetra)):
    array_grises = valoresPorLetra.get(letras[i])
    tamAlfabeto += 1
    if len(array_grises) == 0:
      break
  return tamAlfabeto

def read_images(txt): # txt: archivo.txt
  labels = np.array([])

  # Lectura de nombres de archivos de entrenamiento
  path_base = PATH_BASE
  f = open(path_base + "000/" + txt,"r")
  lineas = f.readlines()

  imagenes = []
  for i in range(1, len(lineas)):
    nombreArchivo = lineas[i].split()[0]
    labels = np.append(labels, lineas[i].split()[1])
    path = path_base + "images/" + nombreArchivo
    img = cv2.imread(path)
    imagenes.append(img)
  return imagenes, labels # retorna un array de imágenes y un array de nombre de clases

def get_character(n, characters, _range):
  # n es el valor del pixel
  indice = int(n // _range)
  return characters[indice]

def extract_patches(image, patch_size, step):
  if WITH_ROTATION:
    return extract_patches_rotation(image, patch_size, step)
  else: 
    view = view_as_windows(image, patch_size, step) # crea ventanas cuadradas
    return view.reshape(-1,patch_size[0]*patch_size[1]) # convierte cada ventana en un vector

def extract_patches_rotation(image, patch_size, step):
  if patch_size[0] == 3 and patch_size[1] == 3:
    view = view_as_windows(image, patch_size, step) # crea ventanas cuadradas

    wiew_rotation = []
    for w in view[0]:
      wiew_rotation.append(w) 
      for i in range(len(NEIGHBORS_LIST)):
        w_copy = w.copy()
        row, column = NEIGHBORS_LIST[i]
        temp = w_copy[1][1]
        w_copy[1][1] = w_copy[row][column]
        w_copy[row][column] = temp
        wiew_rotation.append(w_copy)

    return np.array([wiew_rotation]).reshape(-1,patch_size[0]*patch_size[1]) # convierte cada ventana en un vector
  else:
    raise Exception("Window size must be (3,3)")

def preprocessing(image):
  # Dividir la imagen en sus tres canales R, G, B
  b,g,r = cv2.split(image)

  # Inicializar kernel de TOPHAT
  if PREPROCESSING == 2 or PREPROCESSING == 4 or PREPROCESSING == 6:
    # Obtener el kernel a ser usado en Top-Hat 
    filterSize =(3, 3) 
    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, filterSize) 

  # Preprocesamiento con HSV
  if PREPROCESSING == 1 or PREPROCESSING == 2:
      # Pasar a HSV
      hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)
      h, s, v = cv2.split(hsv_image)
  if PREPROCESSING == 1:
      # HE
      # Operación HE al canal V (value)
      equ_v = cv2.equalizeHist(v)
      # merge de los 3 canales
      hsv_image_final = cv2.merge([h, s, equ_v])
  elif PREPROCESSING == 2:
      # TOP-HAT
      # Operación tophat al canal V (value)
      tophat_v = cv2.morphologyEx(v, cv2.MORPH_TOPHAT, kernel)
      # merge de los 3 canales
      hsv_image_final = cv2.merge([h, s, tophat_v])

  if PREPROCESSING == 1 or PREPROCESSING == 2:  
    # volver a convertir a rgb
    rgb_final = cv2.cvtColor(hsv_image_final, cv2.COLOR_HSV2BGR)
    b,g,r = cv2.split(rgb_final)

  # Preprocesamiento con HSL
  if PREPROCESSING == 5 or PREPROCESSING == 6:
      # Pasar a HSL
      hsl_image = cv2.cvtColor(image, cv2.COLOR_BGR2HLS)
      h, l, s = cv2.split(hsl_image)
  if PREPROCESSING == 5:
      # HE
      # Operación HE al canal L
      equ_l = cv2.equalizeHist(l)
      # merge de los 3 canales
      hsl_image_final = cv2.merge([h, equ_l, s])
  elif PREPROCESSING == 6:
      # TOP-HAT
      # Operación tophat al canal L
      tophat_l = cv2.morphologyEx(l, cv2.MORPH_TOPHAT, kernel)
      # merge de los 3 canales
      hsl_image_final = cv2.merge([h, tophat_l, s])

  if PREPROCESSING == 5 or PREPROCESSING == 6:  
    # volver a convertir a rgb
    rgb_final = cv2.cvtColor(hsl_image_final, cv2.COLOR_HLS2BGR)
    b,g,r = cv2.split(rgb_final)

  # Preprocesamiento con RGB
  if PREPROCESSING == 3:
    b = cv2.equalizeHist(b)
    g = cv2.equalizeHist(g)
    r = cv2.equalizeHist(r)
  elif PREPROCESSING == 4:
    b = cv2.morphologyEx(b, cv2.MORPH_TOPHAT, kernel)
    g = cv2.morphologyEx(g, cv2.MORPH_TOPHAT, kernel)
    r = cv2.morphologyEx(r, cv2.MORPH_TOPHAT, kernel)

  return b, g, r

def method1(imagenes, letras, windows, step, _range):
  cantidad_imagenes = len(imagenes)

  palabras_por_imagen = []
  # Variables utilizadas para calcular posteriormente cantidades / tasas (para añadir nuevos valores al vector de características)
  palabras_por_imagen_B = []
  palabras_por_imagen_G = []
  palabras_por_imagen_R = []

  # Recorrer todas las imágenes
  for imagen in range(cantidad_imagenes):
    palabras_por_imagen.append('')
    palabras_por_imagen_B.append('')
    palabras_por_imagen_G.append('')
    palabras_por_imagen_R.append('')
    
    # Dividir la imagen en sus tres canales
    b,g,r = preprocessing(imagenes[imagen])

    # Generar palabras una ventana a la vez
    for VENTANA in windows:
      palabras = []
      palabras_B = []
      palabras_G = []
      palabras_R = []
      
      # Extracción de las ventanas de la imagen (por cada canal)
      # AZUL
      ventanas_b = extract_patches(b, VENTANA, step)
      # VERDE
      ventanas_g = extract_patches(g, VENTANA, step)
      # ROJO
      ventanas_r = extract_patches(r, VENTANA, step)
      
      longitud_ventanas = len(ventanas_b) # Misma longitud para todas las ventanas
      # Por cada ventana extraída
      for i in range(longitud_ventanas):
        palabras.append('')
        palabras_B.append('')
        palabras_G.append('')
        palabras_R.append('')

        for j in range(len(ventanas_b[i])): # por cada pixel en la ventana B
          palabras_B[i] = palabras_B[i] + get_character(ventanas_b[i][j], letras, _range)
          palabras[i] = palabras[i] + get_character(ventanas_b[i][j], letras, _range) # se obtiene el caracter que representa al valor de gris de ese pixel

        for j in range(len(ventanas_g[i])): # por cada pixel en la ventana G
          palabras_G[i] = palabras_G[i] + get_character(ventanas_g[i][j], letras, _range)
          palabras[i] = palabras[i] + get_character(ventanas_g[i][j], letras, _range) # se obtiene el caracter que representa al valor de gris de ese pixel
        
        for j in range(len(ventanas_r[i])): # por cada pixel en la ventana R
          palabras_R[i] = palabras_R[i] + get_character(ventanas_r[i][j], letras, _range)
          palabras[i] = palabras[i] + get_character(ventanas_r[i][j], letras, _range) # se obtiene el caracter que representa al valor de gris de ese pixel

      # Concatenar en la posición "imagen" correspondiente dentro de los arrays de palabras
      if len(palabras_por_imagen[imagen]) > 0:
        palabras_por_imagen[imagen] = palabras_por_imagen[imagen] + ' ' + ' '.join(palabras) # corpus para el CountVectorizer. Una lista de strings, donde las palabras están separadas por espacios
        palabras_por_imagen_B[imagen] = palabras_por_imagen_B[imagen] + ' ' + ' '.join(palabras_B)
        palabras_por_imagen_G[imagen] = palabras_por_imagen_G[imagen] + ' ' + ' '.join(palabras_G)
        palabras_por_imagen_R[imagen] = palabras_por_imagen_R[imagen] + ' ' + ' '.join(palabras_R)
      else:
        palabras_por_imagen[imagen] = ' '.join(palabras)
        palabras_por_imagen_B[imagen] = ' '.join(palabras_B)
        palabras_por_imagen_G[imagen] = ' '.join(palabras_G)
        palabras_por_imagen_R[imagen] = ' '.join(palabras_R)
  
  return palabras_por_imagen, palabras_por_imagen_B, palabras_por_imagen_G, palabras_por_imagen_R

def method2(imagenes, letras, windows, step, _range):
  cantidad_imagenes = len(imagenes)
  palabras_por_imagen = []
  palabras_por_imagen_R = []
  palabras_por_imagen_G = []
  palabras_por_imagen_B = []

  for imagen in range(cantidad_imagenes):
    palabrasR = [] # palabras del plano R
    palabrasG = [] # palabras del plano G
    palabrasB = [] # palabras del plano B

    b,g,r = preprocessing(imagenes[imagen])
    for v in range(len(windows)):
      ventanasR = extract_patches(r, windows[v], step) # se extraen las ventanas de la imagen en el plano R
      ventanasG = extract_patches(g, windows[v], step) # se extraen las ventanas de la imagen en el plano G
      ventanasB = extract_patches(b, windows[v], step) # se extraen las ventanas de la imagen en el plano B
      palabras_length=len(palabrasB)
      for i in range(len(ventanasB)): # por cada ventana de la imagen
        palabrasR.append("")
        palabrasG.append("")
        palabrasB.append("")
        i_index = i+palabras_length
        for j in range(len(ventanasB[i])): # por cada pixel en la ventana
          palabrasR[i_index] = palabrasR[i_index] + get_character(ventanasR[i][j], letras, _range) # se obtiene el caracter que representa al valor de gris de ese pixel
          palabrasG[i_index] = palabrasG[i_index] + get_character(ventanasG[i][j], letras, _range) # se obtiene el caracter que representa al valor de gris de ese pixel
          palabrasB[i_index] = palabrasB[i_index] + get_character(ventanasB[i][j], letras, _range) # se obtiene el caracter que representa al valor de gris de ese pixel
        palabrasR[i_index]=palabrasR[i_index] +  ALPHA
        palabrasG[i_index]=palabrasG[i_index] + BETA
        palabrasB[i_index]=palabrasB[i_index] + GAMMA

    palabras_por_imagen.append(' '.join(palabrasR) +" "+ ' '.join(palabrasG) +" "+ ' '.join(palabrasB)) # corpus para el CountVectorizer. Una lista de strings, donde las palabras están separadas por espacios
    palabras_por_imagen_R.append(' '.join(palabrasR))
    palabras_por_imagen_G.append(' '.join(palabrasG))
    palabras_por_imagen_B.append(' '.join(palabrasB))

  return palabras_por_imagen, palabras_por_imagen_R, palabras_por_imagen_G, palabras_por_imagen_B


def extract_words(imagenes, characters, windows, step, _range):
  if METHOD == 1:
    palabras_por_imagen, palabras_por_imagen_B, palabras_por_imagen_G, palabras_por_imagen_R = method1(imagenes, characters, windows, step, _range)
  elif METHOD == 2:
    palabras_por_imagen, palabras_por_imagen_B, palabras_por_imagen_G, palabras_por_imagen_R = method2(imagenes, characters, windows, step, _range)
  return palabras_por_imagen, palabras_por_imagen_B, palabras_por_imagen_G, palabras_por_imagen_R

# diversidad léxica
def herdan(V, N):
  return math.log(V, 10) / math.log(N, 10)

def somers(V, N):
  num = math.log(math.log(V, 10), 10)
  den = math.log(math.log(N, 10), 10)
  return num / den

def honore(V, N, V1):
  num = math.log(N, 10)
  den = 1 - (V1 / V)
  return 100 * (num / den)

def maas(V, N):
  num = math.log(N, 10) - math.log(V, 10)
  den = math.log(V, 10)**2
  return num/den

def dugast(V, N):
  num = math.log(N, 10)**2
  den = math.log(N, 10) - math.log(V, 10)
  return num/den

# Inter-channel measures
def normalized_euclidean_distance(Qi, Qj, histograma):
  std = np.std(histograma)  
  return math.sqrt(((Qi - Qj) / std)**2)

def canberra(Qi, Qj):
  num = abs(Qi-Qj)
  den = Qi+Qj
  if den == 0:
    canberra = 0
  else:
    canberra = num/den
  return canberra

def add_values_histogram(vectorizer, X, X_B, X_G, X_R):
  cantidad_imagenes = X.shape[0]
  
  maxi = (max(vectorizer.vocabulary_.items(), key=operator.itemgetter(1))[1] + 1) + SUM_LEXICAL_DIVERSITY + SUM_STATISTICS + SUM_NORMALIZED_EUCLIDEAN_DISTANCE + SUM_CANBERRA_DISTANCE + SUM_QUANTITIES
  X = csr_matrix(X, (cantidad_imagenes, maxi), dtype=np.float64)

  if SUM_LEXICAL_DIVERSITY > 0:
    sum_columns = X.sum(axis=1)

  # Añadir cantidades al vector de características
  for i in range(cantidad_imagenes):
    substracter = 1
    # CANTIDADES
    sum_histograma = X[i].count_nonzero() # cantidad de palabras total del diccionario
    sum_b = X_B[i].count_nonzero()
    sum_g = X_G[i].count_nonzero()
    sum_r = X_R[i].count_nonzero()

    X_histograma = X[i].A[0] #histograma de la imagen i

    # QUANTITIES
    if SUM_QUANTITIES > 0:
      if QUANTITIES.get("RGB"):
        X[i, maxi - substracter] = sum_histograma
        substracter += 1
      if QUANTITIES.get("B"):
        X[i, maxi - substracter] = sum_b
        substracter += 1
      if QUANTITIES.get("G"):
        X[i, maxi - substracter] = sum_g
        substracter += 1
      if QUANTITIES.get("R"):
        X[i, maxi - substracter] = sum_r
        substracter += 1
      
    # LEXICAL DIVERSITY:
    if SUM_LEXICAL_DIVERSITY > 0:
      distintas = X[i].shape[1] 
      V = 0 # palabras distintas del texto
      for j in range(distintas):
        if X[i, j] > 0:
          V += 1
      N = sum_columns[i] # palabras (suma total de palabras)
      V1 = 0 # palabras que aparecen una sola vez
      for j in range(distintas):
        if X[i, j] == 1:
          V1 += 1

      # TTR
      if LEXICAL_DIVERSITY.get("TTR"):
        X[i, maxi - substracter] = V / N
        substracter += 1
      # Herdan
      if LEXICAL_DIVERSITY.get("HERDAN"):
        X[i, maxi - substracter] = herdan(V, N)
        substracter += 1
      # Somers
      if LEXICAL_DIVERSITY.get("SOMERS"):
        X[i, maxi - substracter] = somers(V, N)
        substracter += 1
      # Honoré
      if LEXICAL_DIVERSITY.get("HONORE"):
        X[i, maxi - substracter] = honore(V, N, V1)
        substracter += 1
      # Maas
      if LEXICAL_DIVERSITY.get("MAAS"):
        X[i, maxi - substracter] = maas(V, N)
        substracter += 1
      # Dugast
      if LEXICAL_DIVERSITY.get("DUGAST"):
        X[i, maxi - substracter] = dugast(V, N)
        substracter += 1

    # STATISTICS:
    if SUM_STATISTICS > 0:
      # desviación estándar 
      if STATISTICS.get("STD"):
        X[i, maxi - substracter] = np.std(X_histograma)
        substracter += 1
      # Mediana
      if STATISTICS.get("MEDIAN"):
        X[i, maxi - substracter] = np.median(X_histograma)
        substracter += 1
      # Varianza
      if STATISTICS.get("VARIANCE"):
        X[i, maxi - substracter] = np.var(X_histograma)
        substracter += 1
      # Rango
      if STATISTICS.get("RANGE"):
        X[i, maxi - substracter] = max(X_histograma)-min(X_histograma)
        substracter += 1
      # Entropía
      if STATISTICS.get("ENTROPY"):
        X[i, maxi - substracter] = st.entropy(X_histograma)
        substracter += 1
      # Media aritmética
      if STATISTICS.get("MEAN"):
        X[i, maxi - substracter] = X_histograma.mean()
        substracter += 1
      # Moda
      if STATISTICS.get("MODE"):
        X[i, maxi - substracter] = st.mode(X_histograma)[0][0]
        substracter += 1
      # Desviación absoluta mediana (o media) - MAD
      if STATISTICS.get("MAD"):
        X[i, maxi - substracter] = st.median_absolute_deviation(X_histograma)
        substracter += 1

    # Tasas:
    if SUM_RATES > 0:
      if RATES.get("BLUE_RGB"):
        # Azul / Total
        X[i, maxi - substracter] = (sum_b / sum_histograma) if sum_histograma > 0 else 0
        substracter += 1
      if RATES.get("GREEN_RGB"):
        # Verde / Total
        X[i, maxi - substracter] = (sum_g / sum_histograma) if sum_histograma > 0 else 0
        substracter += 1
      if RATES.get("RED_RGB"):
        # Rojo / Total 
        X[i, maxi - substracter] = (sum_r / sum_histograma) if sum_histograma > 0 else 0
        substracter += 1
      if RATES.get("RED_GREEN"):
        # Rojo / Verde
        X[i, maxi - substracter] = (sum_r / sum_g) if sum_g > 0 else 0
        substracter += 1
      if RATES.get("RED_BLUE"):
        # Rojo / Azul
        X[i, maxi - substracter] = (sum_r / sum_b) if sum_b > 0 else 0
        substracter += 1
      if RATES.get("GREEN_RED"):
        # Verde / Rojo
        X[i, maxi - substracter] = (sum_g / sum_r) if sum_r > 0 else 0
        substracter += 1
      if RATES.get("GREEN_BLUE"):
        # Verde / Azul
        X[i, maxi - substracter] = (sum_g / sum_b) if sum_b > 0 else 0
        substracter += 1
      if RATES.get("BLUE_RED"):
        # Azul / rojo
        X[i, maxi - substracter] = (sum_b / sum_r) if sum_r > 0 else 0
        substracter += 1
      if RATES.get("BLUE_GREEN"):
        # Azul / verde
        X[i, maxi - substracter] = (sum_b / sum_g) if sum_g > 0 else 0
        substracter += 1

    # INTER-CHANNEL MEASURES
    # Normalized Euclidean Distance
    if SUM_NORMALIZED_EUCLIDEAN_DISTANCE > 0:
      # rojo, verde
      if NORMALIZED_EUCLIDEAN_DISTANCE.get("RED_GREEN"):
        X[i, maxi - substracter] = normalized_euclidean_distance(sum_r, sum_g, X_histograma)
        substracter += 1
      # verde, azul
      if NORMALIZED_EUCLIDEAN_DISTANCE.get("GREEN_BLUE"):
        X[i, maxi - substracter] = normalized_euclidean_distance(sum_g, sum_b, X_histograma)
        substracter += 1
      # azul, rojo
      if NORMALIZED_EUCLIDEAN_DISTANCE.get("RED_BLUE"):
        X[i, maxi - substracter] = normalized_euclidean_distance(sum_b, sum_r, X_histograma)
        substracter += 1

    # Canberra distance
    if SUM_CANBERRA_DISTANCE > 0:
      # Azul a Total
      if CANBERRA_DISTANCE.get("BLUE_RGB"):
        X[i, maxi - substracter] = canberra(sum_b, sum_histograma)
        substracter += 1
      # Verde a Total
      if CANBERRA_DISTANCE.get("GREEN_RGB"):
        X[i, maxi - substracter] = canberra(sum_g, sum_histograma)
        substracter += 1
      # Rojo a Total 
      if CANBERRA_DISTANCE.get("RED_RGB"):
        X[i, maxi - substracter] = canberra(sum_r, sum_histograma)
        substracter += 1
      # Rojo a Verde
      if CANBERRA_DISTANCE.get("RED_GREEN"):
        X[i, maxi - substracter] = canberra(sum_r, sum_g)
        substracter += 1
      # Rojo a Azul
      if CANBERRA_DISTANCE.get("RED_BLUE"):
        X[i, maxi - substracter] = canberra(sum_r, sum_b)
        substracter += 1
      # Verde a Azul
      if CANBERRA_DISTANCE.get("GREEN_BLUE"):
        X[i, maxi - substracter] = canberra(sum_g, sum_b)
        substracter += 1

  return X

def train(imagenes, labels_entrenamiento, characters, windows, step, _range):
  palabras_por_imagen, palabras_por_imagen_B, palabras_por_imagen_G, palabras_por_imagen_R = extract_words(imagenes, characters, windows, step, _range)
  print ('palabras_por_imagen', palabras_por_imagen[0])

  #Generar diccionario
  vectorizer = CountVectorizer(token_pattern=r"(?u)[" + characters + ALPHA + BETA + GAMMA + "]+", lowercase=False) # Nos indica con qué caracteres están conformadas las palabras.
  X = vectorizer.fit_transform(palabras_por_imagen)
  print("despues de countvectorizer")
  diccionario = vectorizer.get_feature_names() # diccionario

  tam_diccionario = len(diccionario) # Tamaño del diccionario / alfabeto

  vectorizer_B = CountVectorizer(token_pattern=r"(?u)[" + characters + ALPHA + BETA + GAMMA + "]+", lowercase=False)
  X_B = vectorizer_B.fit_transform(palabras_por_imagen_B)
  vectorizer_G = CountVectorizer(token_pattern=r"(?u)[" + characters + ALPHA + BETA + GAMMA + "]+", lowercase=False)
  X_G = vectorizer_G.fit_transform(palabras_por_imagen_G)
  vectorizer_R = CountVectorizer(token_pattern=r"(?u)[" + characters + ALPHA + BETA + GAMMA + "]+", lowercase=False)
  X_R = vectorizer_R.fit_transform(palabras_por_imagen_R)

  # agregar al histograma
  X = add_values_histogram(vectorizer, X, X_B, X_G, X_R)

  # creación archivo de vector de caracteristicas
  if FORMAT_VECT_CAR == 'csv':
    with open(CSV_ENTRENAMIENTO,'w+') as f2:
      w = csv.writer(f2)
      titulos = ["CLASE"] + vectorizer.get_feature_names()
      w.writerow(titulos)
      for i in range(len(X.toarray())):
        fila = []
        fila.append(labels_entrenamiento[i])
        fila = fila + X[i].toarray().tolist()[0]
        w.writerow(fila)
  elif FORMAT_VECT_CAR == 'npz':
    scipy.sparse.save_npz(CSV_ENTRENAMIENTO, X)

  # Entrenar clasificador
  print("antes de entrenar")
  if CLASSIFIER == 'ovsr':
    svm = OneVsRestClassifier(SVC(kernel = 'linear'))
    svm.fit(X, labels_entrenamiento)
    _classifier = svm
  elif CLASSIFIER == 'svc':
    svm = SVC(kernel = 'linear')
    svm.fit(X, labels_entrenamiento)
    _classifier = svm
  elif CLASSIFIER == 'knn':
    n_neighbors = 1
    knn = KNeighborsClassifier(n_neighbors)
    knn.fit(X, labels_entrenamiento)
    _classifier = knn
  
  #return svm, vectorizer, vectorizer_B, vectorizer_G, vectorizer_R, tam_diccionario
  return _classifier, vectorizer, vectorizer_B, vectorizer_G, vectorizer_R, tam_diccionario

def test(imagenes, labels_prueba, _classifier, vectorizer, vectorizer_B, vectorizer_G, vectorizer_R, characters, windows, step, _range):
  # Extracción de palabras de las imágenes de prueba
  palabras_por_imagen, palabras_por_imagen_R, palabras_por_imagen_G, palabras_por_imagen_B = extract_words(imagenes, characters, windows, step, _range)
  print ('palabras_por_imagen', palabras_por_imagen[0])

  # Generación de histogramas de las imágenes de prueba
  histogramas_img_prueba = vectorizer.transform(palabras_por_imagen)

  # 3 canales
  X_B = vectorizer_B.transform(palabras_por_imagen_B)
  X_G = vectorizer_G.transform(palabras_por_imagen_G)
  X_R = vectorizer_R.transform(palabras_por_imagen_R)

  # agregar al histograma
  histogramas_img_prueba = add_values_histogram(vectorizer, histogramas_img_prueba, X_B, X_G, X_R)

  # guardar vector de características - prueba
  if FORMAT_VECT_CAR == 'csv':
    with open(CSV_PRUEBA,'w+') as f2:
      w = csv.writer(f2)
      titulos = ["CLASE"] + vectorizer.get_feature_names()
      w.writerow(titulos)
      for i in range(len(histogramas_img_prueba.toarray())):
        fila = []
        fila.append(labels_prueba[i])
        fila = fila + histogramas_img_prueba[i].toarray().tolist()[0]
        w.writerow(fila)
  elif FORMAT_VECT_CAR == 'npz':
    scipy.sparse.save_npz(CSV_PRUEBA, histogramas_img_prueba)

  print("antes de clasficador - test")
  predicciones = _classifier.predict(histogramas_img_prueba)

  return predicciones

def results(labels_prueba, predicciones):
  exactitud = accuracy_score(labels_prueba, predicciones)

  return exactitud

def experiment(_range, windows, step, imagenes_entrenamiento, labels_entrenamiento, imagenes_prueba, labels_prueba, characters):
  print("train...")
  #svm, vectorizer, vectorizer_B, vectorizer_G, vectorizer_R, tam_diccionario = train(imagenes_entrenamiento, labels_entrenamiento, characters, windows, step, _range)
  _classifier, vectorizer, vectorizer_B, vectorizer_G, vectorizer_R, tam_diccionario = train(imagenes_entrenamiento, labels_entrenamiento, characters, windows, step, _range)
  print("test...")
  #predicciones = test(imagenes_prueba, labels_prueba, svm, vectorizer, vectorizer_B, vectorizer_G, vectorizer_R, characters, windows, step, _range)
  predicciones = test(imagenes_prueba, labels_prueba, _classifier, vectorizer, vectorizer_B, vectorizer_G, vectorizer_R, characters, windows, step, _range)
  exactitud = results(labels_prueba, predicciones)

  return exactitud, tam_diccionario

params_set = generate_configurations()
print(params_set)
characters = generate_characters()

#Lectura de imágenes
imagenes_entrenamiento, labels_entrenamiento =  read_images("train.txt")
print("Cantidad de imágenes de entrenamiento:",  len(imagenes_entrenamiento))
imagenes_prueba, labels_prueba =  read_images("test.txt")
print("Cantidad de imágenes de prueba:",  len(imagenes_prueba))

titulos = ["RANGO", "VENTANA", "STEP", "Exactitud", "Tamaño Alfabeto", "Tamaño Diccionario"] # Para imprimir en csv

with open(CSV_PATH,'w') as f:
  w = csv.writer(f)
  w.writerow(titulos)

  for param in params_set:
      _range = param.get("range")
      windows = param.get("windows")
      step = param.get("step")
      
      print(windows)
      
      tamAlfabeto = get_alphabet_size(characters, _range)
      exactitud, tam_diccionario = experiment(_range, windows, step, imagenes_entrenamiento, labels_entrenamiento, imagenes_prueba, labels_prueba, characters)
      w.writerow([_range, windows, step, exactitud, tamAlfabeto, tam_diccionario])

      resultados = "RANGO :" + str(_range) + " VENTANA :" + str(windows) + " STEP :" + str(step) + " exactitud : " + str(exactitud) + " tam_alfabeto : " + str(tamAlfabeto) + " tam_diccionario :" + str(tam_diccionario)
      print(colored(resultados, 'red'))
