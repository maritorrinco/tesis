# -*- coding: utf-8 -*-
"""Metodo propuesto - escala de grises.ipynb

Automatically generated by Colaboratory.

Original file is located at
		https://colab.research.google.com/drive/1QBNb0SVighc3VgfcjxHn5Q6UV0QoQZWr
"""

from skimage.util import view_as_windows
import cv2
from sklearn.feature_extraction.text import CountVectorizer
import numpy as np
from sklearn.svm import SVC
from sklearn.multiclass import OneVsRestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import MultinomialNB, GaussianNB
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score
from itertools import product
from termcolor import colored
import csv
import sys
import scipy.sparse
from sklearn.preprocessing import MinMaxScaler


################################### Parámetros ###################################
print("Generación de parámetros....")
BASE_DATOS_PARAM = "Outex_TC_00013"
if len(sys.argv) > 1:
	BASE_DATOS_PARAM = sys.argv[1] # nombre de la base de datos a usar

parametros = {'rango': [10],
							'v1': [2],
							'v2': [2],
							'step': [1]}
if len(sys.argv) > 2:
	parametros["rango"] = [int(sys.argv[2])]
if len(sys.argv) > 3:
	parametros["v1"] = [int(sys.argv[3])]
if len(sys.argv) > 4:
	parametros["v2"] = [int(sys.argv[4])]
param_names, param_values = zip(*parametros.items())
parametros_set = [dict(zip(param_names, h)) for h in product(*param_values)]

CLASIFICADOR = 'svc' # Posibles valores: ovsr; svc; knn; rf
if len(sys.argv) > 5:
	CLASIFICADOR = sys.argv[5]

FORMAT_VECT_CAR = 'npz' # Posibles valores: csv; npz
if len(sys.argv) > 6:
	FORMAT_VECT_CAR = sys.argv[6]

print(BASE_DATOS_PARAM, parametros_set, CLASIFICADOR,  FORMAT_VECT_CAR)

if CLASIFICADOR == "mnb":
	scaler = MinMaxScaler()


########################################## Constantes ######################################
PATH_BASE = "../databases/{}/".format(BASE_DATOS_PARAM)
CSV_PRUEBA = "../resultados/metodo_propuesto_escala_de_grises/vector_prueba_{bd}.{extension}".format(bd=BASE_DATOS_PARAM, extension=FORMAT_VECT_CAR)
CSV_ENTRENAMIENTO =  "../resultados/metodo_propuesto_escala_de_grises/vector_entrenamiento_{bd}.{extension}".format(bd=BASE_DATOS_PARAM, extension=FORMAT_VECT_CAR)
CSV_EXACTITUD = "../resultados/metodo_propuesto_escala_de_grises/exactitud_{bd}_{clasificador}.csv".format(bd=BASE_DATOS_PARAM, clasificador=CLASIFICADOR)


############################################ Funciones #####################################
def getLetra(n, letras):
	# n es el valor del pixel
	indice = int(n // RANGO)
	return letras[indice]

def extract_patches(image, patch_size, step):
	view = view_as_windows(image, patch_size, step) # crea ventanas cuadradas 

	return view.reshape(-1,patch_size[0]*patch_size[1]) # convierte cada ventana en un vector

def leer_imagenes(txt): # txt: archivo.txt
	labels = np.array([])

	# Lectura de nombres de archivos de entrenamiento
	f = open(PATH_BASE + "000/" + txt,"r")
	lineas = f.readlines()

	imagenes = []
	for i in range(1, len(lineas)):
		nombreArchivo = lineas[i].split()[0]
		labels = np.append(labels, lineas[i].split()[1])
		path = PATH_BASE + "images/" + nombreArchivo
		img = cv2.imread(path)
		img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
		imagenes.append(img_gray)
	return imagenes, labels # retorna un array de imágenes y un array de nombre de clases

def extraer_palabras(imagenes, letras):
	cantidad_imagenes = len(imagenes)
	palabras_por_imagen = []
	
	for imagen in range(cantidad_imagenes):
		ventanas = extract_patches(imagenes[imagen], VENTANA, STEP) # se extraen las ventanas de la imagen
		palabras = []
		for i in range(len(ventanas)): # por cada ventana de la iamgen
			palabras.append("")
			for j in range(len(ventanas[i])): # por cada pixel en la ventana
				palabras[i] = palabras[i] + getLetra(ventanas[i][j], letras) # se obtiene el caracter que representa al valor de gris de ese pixel
		palabras_por_imagen.append(' '.join(palabras)) # corpus para el CountVectorizer. Una lista de strings, donde las palabras están separadas por espacios
	
	return palabras_por_imagen

def generar_letras():
	letras = ""

	start = 0x0021 # desde !
	end = 0x007E # hasta ~
	for i in range(start, end + 1):
		letras = letras+chr(i)

	start_1 = 0x00C0 # desde À
	end_1 = 0x00FF # hasta ÿ
	for i in range(start_1, end_1 + 1):
		letras = letras+chr(i)

	start_2 = 0x0100 # Ā
	end_2 = 0x0161 # š
	for i in range(start_2, end_2 + 1):
		letras = letras+chr(i)
	return letras

def mostrar_alfabeto(letras):
	p1 = 0
	p2 = 0
	valoresPorLetra = {} # Para mostrar
	for i in range(len(letras)):
		p2 = p1 + (RANGO - 1)
		if p2 > 255:
			p2 = 255 
		valoresPorLetra[letras[i]] = list(range(p1, p2 + 1))
		p1 = p1 + RANGO

	tamAlfabeto = 0
	for i in range(len(valoresPorLetra)):
		array_grises = valoresPorLetra.get(letras[i])
		if len(array_grises) == 0:
			break
		tamAlfabeto += 1
		print(letras[i], "->", array_grises)

	print("Tamaño del Alfabeto:", tamAlfabeto)
	return tamAlfabeto

def entrenamiento(imagenes_entrenamiento, labels_entrenamiento, letras):
	# Extracción de palabras de las imágenes de entrenamiento
	palabras_por_imagen_entrenamiento = extraer_palabras(imagenes_entrenamiento, letras)
	
	#Generar diccionario
	vectorizer = CountVectorizer(token_pattern=r"(?u)[" + letras + "]+", lowercase=False) # Nos indica con qué caracteres están conformadas las palabras.
	# Obtener los histogramas
	X = vectorizer.fit_transform(palabras_por_imagen_entrenamiento)

	diccionario = vectorizer.get_feature_names() # diccionario
	tam_diccionario = len(diccionario) # Tamaño del diccionario / alfabeto

	# creación archivo de vector de caracteristicas
	if FORMAT_VECT_CAR == 'csv':
		with open(CSV_ENTRENAMIENTO,'w') as f2:
			w = csv.writer(f2)
			titulos = ["CLASE"] + vectorizer.get_feature_names()
			w.writerow(titulos)
			for i in range(len(X.toarray())):
				fila = []
				fila.append(labels_entrenamiento[i])
				fila = fila + X[i].toarray().tolist()[0]
				w.writerow(fila)
	elif FORMAT_VECT_CAR == 'npz':
		scipy.sparse.save_npz(CSV_ENTRENAMIENTO, X)
	
	if CLASIFICADOR == "svc":
		classifier = SVC(kernel = 'linear')
	elif CLASIFICADOR == "knn":
		classifier = KNeighborsClassifier(n_neighbors = 1)
	elif CLASIFICADOR == "ovsr":
		classifier = OneVsRestClassifier(SVC(kernel = 'linear'))
	elif CLASIFICADOR == "rf":
		classifier = RandomForestClassifier(random_state = 0)
	elif CLASIFICADOR == "nb":
		classifier = GaussianNB()
	elif CLASIFICADOR == "mnb":
		classifier = MultinomialNB()
		#X = scaler.fit_transform(X.toarray())
	elif CLASIFICADOR == "mlp":
    	classifier = MLPClassifier(random_state=0, max_iter=500)
		
	# Entrenar clasificador
	#classifier.fit(X.toarray(), labels_entrenamiento)
	classifier.fit(X, labels_entrenamiento)
	
	return classifier, vectorizer, tam_diccionario

def prueba(imagenes_prueba, labels_prueba, classifier, vectorizer, letras):
	
	# Extracción de palabras de las imágenes de prueba
	palabras_por_imagen_prueba = extraer_palabras(imagenes_prueba, letras)

	# Generación de histogramas de las imágenes de prueba
	histogramas_img_prueba = vectorizer.transform(palabras_por_imagen_prueba)
	
	# guardar vector de características - prueba
	if FORMAT_VECT_CAR == 'csv':
		with open(CSV_PRUEBA,'w') as f2:
			w = csv.writer(f2)
			titulos = ["CLASE"] + vectorizer.get_feature_names()
			w.writerow(titulos)
			for i in range(len(histogramas_img_prueba.toarray())):
				fila = []
				fila.append(labels_prueba[i])
				fila = fila + histogramas_img_prueba[i].toarray().tolist()[0]
				w.writerow(fila)
	elif FORMAT_VECT_CAR == 'npz':
		scipy.sparse.save_npz(CSV_PRUEBA, histogramas_img_prueba)

	# Predicciones
	#if CLASIFICADOR == "mnb":
		#histogramas_img_prueba = scaler.fit_transform(histogramas_img_prueba.toarray())
	#predicciones = classifier.predict(histogramas_img_prueba.toarray())
	predicciones = classifier.predict(histogramas_img_prueba)
	# print("Predicciones:\n", predicciones)

	# Resultados
	exactitud = accuracy_score(labels_prueba, predicciones)
	print ('Exactitud: %0.3f' % exactitud)
	return exactitud

def main(RANGO, VENTANA, STEP, imagenes_entrenamiento, labels_entrenamiento, imagenes_prueba, labels_prueba, letras):
	print("Entrenamiento...")
	classifier, vectorizer, tam_diccionario = entrenamiento(imagenes_entrenamiento, labels_entrenamiento, letras)

	print("Prueba...")
	exactitud = prueba(imagenes_prueba, labels_prueba, classifier, vectorizer, letras)
	return exactitud, tam_diccionario


#################################### EJECUTAR ##########################################
#Leer imágenes de entrenamiento (una sola vez)
print("Lectura de imágenes de entrenamiento....")
imagenes_entrenamiento, labels_entrenamiento =  leer_imagenes("train.txt")
print("Cantidad de imágenes de entrenamiento:",  len(imagenes_entrenamiento))

# Lectura de imágenes de prueba (una sola vez)
print("Lectura de imágenes de prueba....")
imagenes_prueba, labels_prueba =  leer_imagenes("test.txt")
print("Cantidad de imágenes de prueba:",  len(imagenes_prueba))

# Genera un alfabeto de 256 caracteres
print("Generación del alfabeto....")
letras = generar_letras()

titulos = ["Descriptor", "Escala", "Base de datos", "Clasificador", "Exactitud", "RANGO", "VENTANA", "STEP", "Tamaño Alfabeto", "Tamaño Diccionario"] # Para imprimir en csv
with open(CSV_EXACTITUD,'w') as f:
	w = csv.writer(f)
	w.writerow(titulos)
	
	for param in parametros_set:
		V1 = param.get("v1")
		V2 = param.get("v2")
		RANGO = param.get("rango")
		VENTANA = (V1, V2)
		STEP = param.get("step")

		tamAlfabeto = mostrar_alfabeto(letras)
		exactitud, tam_diccionario = main(RANGO, VENTANA, STEP, imagenes_entrenamiento, labels_entrenamiento, imagenes_prueba, labels_prueba, letras)
		resultados = "RESULTADO -> RANGO :" + str(RANGO) + " VENTANA :" + str(VENTANA) + " STEP :" + str(STEP) + " exactitud : " + str(exactitud) + " tam_alfabeto : " + str(tamAlfabeto) + " tam_diccionario :" + str(tam_diccionario)
		print(colored(resultados, 'red'))

		w.writerow(["Método", "escala de grises", BASE_DATOS_PARAM, CLASIFICADOR, exactitud, RANGO, VENTANA, STEP, tamAlfabeto, tam_diccionario])